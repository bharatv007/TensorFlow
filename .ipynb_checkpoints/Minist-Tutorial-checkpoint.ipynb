{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial on MNIST dataset\n",
    "\n",
    "We would like\n",
    "to train a deep network to classify MNIST dataset into two classes, deciding whether a\n",
    "digit in the image is greater (or equal to) or less than three (3), i.e. digit >=3 or digit<3.\n",
    "### Additional Tasks\n",
    "- Downsample the original images to 14 x 14 pixels\n",
    "- Blurring the images\n",
    "\n",
    "### Questions to be answered\n",
    "- The network architecture must have 1-3 batch-normalized CNN layers (with 3 x 3 x N\n",
    "Kernels, where N is an adjustable parameter) and 2x2 strides followed by a single fully\n",
    "connected layer. \n",
    "\n",
    "- Any other layers or components could be added if required. We would\n",
    "like infer how many CNN layers (1, 2 or 3) results in the best performance of the network\n",
    "based on the database we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharatv/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\")\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, 6, 1, 8, 1, 0, 9, 8], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY2=np.zeros((trX.shape[0],2))\n",
    "trY2[trY>=3,0]=1\n",
    "trY2[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = trX.reshape(-1, 28, 28, 1)  # 28x28x1 input img\n",
    "teX = teX.reshape(-1, 28, 28, 1)  # 28x28x1 input img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teY2=np.zeros((teX.shape[0],2))\n",
    "teY2[teY>=3,0]=1\n",
    "teY2[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blurring the image using Tensor flow\n",
    "We multiply two independent 1D Gaussain random variables to obtain a 2D gaussian filter to blur the images. Since the images are small, blurring image filter size is 3x3. We define a tensor flow convolution layer with a predefined filter (gaussian kernel) of diemension 3x3x1x1 such that we do not learn any weights. In our computation graph we add it as an input to our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05854983, 0.09653235, 0.05854983],\n",
       "       [0.09653235, 0.15915494, 0.09653235],\n",
       "       [0.05854983, 0.09653235, 0.05854983]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, k = 1, 1 #  generate a (2k+1)x(2k+1) gaussian kernel with mean=0 and sigma = s\n",
    "probs = [np.exp(-z*z/(2*s*s))/np.sqrt(2*np.pi*s*s) for z in range(-k,k+1)] \n",
    "gaussian_kernel = np.outer(probs, probs)\n",
    "gaussian_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_kernel = np.expand_dims(np.expand_dims(gaussian_kernel, 2), 3)\n",
    "gaussian_kernel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Graph\n",
    "We first begin by creating placeholders for the input data that will be fed into the model when running the session. We have four inputs: X(images),Y(labels), gaussian kernel and phase- an indicator function to tell the graph when batch normalisation is to be used. Batch normalisation works differnely during the training and testing phase of our model. We also initialize our weights to be learned from each layer. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,[None,28,28,1])\n",
    "g_filter = tf.placeholder(dtype=tf.float32, shape=(3, 3, 1, 1))\n",
    "Y=tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "layers=2\n",
    "\n",
    "W1 = tf.get_variable(\"W1\",[3,3,1,16],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "W2 = tf.get_variable(\"W2\",[3,3,16,32],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "W3 = tf.get_variable(\"W3\",[3,3,32,64],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "\n",
    "W=[W1,W2,W3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm\n",
    "Tensor flow has its own batch norm function, but onky works with given parameters mean,var, alpha and beta. We compute the parameters as given in http://arxiv.org/abs/1502.03167 and feed it as arguments to tf.nn.batch_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "def batch_norm(x, n_out, phase_train, scope='bn'):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To generate random Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[(k+1)*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[(k+1)*mini_batch_size:]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "We define one layer of our model to consist of,\n",
    "- A convolution layer\n",
    "- Batch normalization layer\n",
    "- Use relu activation\n",
    "- Followed by Max pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(X,w,s, phase):\n",
    "    \"\"\"\n",
    "    One layer of our network\n",
    "    Args: \n",
    "    X:     Tesnor, 4D inputs\n",
    "    w:     Weights for the layer\n",
    "    s:     Strides on our convolution layer\n",
    "    phase: Indicates if we are training or testing\n",
    "    Returns:\n",
    "    l4:    Output from max pooling layer\n",
    "    \"\"\"\n",
    "    l1=tf.nn.conv2d(X,w,strides=[1,s,s,1],padding=\"SAME\")\n",
    "    # l2=tf.contrib.layers.batch_norm(l1,center=True, scale=True, is_training=phase)\n",
    "    l2 = batch_norm(l1, w.shape[3], phase)\n",
    "    l3=tf.nn.relu(l2)\n",
    "    l4=tf.nn.max_pool(l3,ksize=[1,s,s,1],strides=[1,s,s,1],padding='SAME')\n",
    "    return l4\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "FULLYCONNECTED (FC) layer: We use a fully connected layer without an non-linear activation function. We do not call the softmax here. This will result in 2 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which we'll call in a different function when computing the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X,W,layers, phase,g_filter):\n",
    "    \"\"\"\n",
    "    Model\n",
    "    Args:\n",
    "    X:         input images as batches\n",
    "    W:         All the weights of entire network as a list\n",
    "    layers:    The nmber of layers in our network(1,2,3)\n",
    "    phase:     Indicates if we are training or testing\n",
    "    g_filter:  gaussain kernel\n",
    "    Returns:\n",
    "    out:       Output from the fully conneceted layer\n",
    "    \"\"\"\n",
    "    X_resized=tf.image.resize_images(X,[14,14])\n",
    "    X_blurred=tf.nn.conv2d(X_resized, filter=g_filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    for l in range(layers):\n",
    "        print(W[l])\n",
    "        p=layer(X_blurred,W[l],2, phase)\n",
    "        X_blurred=p\n",
    "    FC1=tf.contrib.layers.flatten(p)\n",
    "    out=tf.layers.dense(FC1, units=2, activation=None) # fully_connected(FC1,10,activation_fn=None)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Network\n",
    "We train our data on three models. The first model has one convolution layer, followed by batch normalization, Max pooling and finally a fully connected layer. The second model has two layers in repetition followed by a fully connected layer. The third model has three layers followed by a fully connected layer. Each of the model the final output is a softmax layer that gives us two classes. Since the data is downsampled to 14x14, the size of the images is small. We also have a skewed data set( more 1 than 0s). We need to make sure that we avoid overfitting as these are small images and fewer data points. So ideally we need small to medium sized network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_model(trX,trY,teX,teY,layers,learning_rate=0.001/2,num_epochs=100,batch_size=128):\n",
    "    tf.reset_default_graph()\n",
    "    X=tf.placeholder(tf.float32,[None,28,28,1])\n",
    "    g_filter = tf.placeholder(dtype=tf.float32, shape=(3, 3, 1, 1))\n",
    "    Y=tf.placeholder(tf.float32,[None,2])\n",
    "\n",
    "    phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\",[3,3,1,8],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W2 = tf.get_variable(\"W2\",[3,3,8,16],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W3 = tf.get_variable(\"W3\",[3,3,16,32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    W=[W1,W2,W3]\n",
    "\n",
    "    num_minibatches=int(trX.shape[0]/batch_size)\n",
    "\n",
    "    #Output from the model\n",
    "    Y_hat=model(X,W,layers, phase,g_filter)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y_hat,labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate Model\n",
    "    predict_op = tf.argmax(Y_hat, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    #Initializer\n",
    "    init=tf.global_variables_initializer()\n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        seed=3\n",
    "        sess.run(init)\n",
    "        for epoch in range(0,num_epochs):\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(trX.shape[0] / batch_size)\n",
    "            seed=seed+1\n",
    "            minibatches=random_mini_batches(trX,trY2,128,seed)\n",
    "            for minibatch in minibatches:\n",
    "                minibatch_x,minibatch_y=minibatch\n",
    "                _,temp_cost=sess.run([optimizer,cost],feed_dict={X:minibatch_x,Y:minibatch_y,phase:True,g_filter:gaussian_kernel})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            if epoch % 10 == 0:\n",
    "                acc=accuracy.eval({X: trX, Y: trY2,phase:False,g_filter:gaussian_kernel})\n",
    "                print (\"Cost after epoch %i: %f and training accuracy: %f\" % (epoch, minibatch_cost,acc))\n",
    "            # Calculate accuracy on the test set\n",
    "        \n",
    "        train_accuracy = accuracy.eval({X: trX, Y: trY2,phase:False,g_filter:gaussian_kernel})\n",
    "        test_accuracy = accuracy.eval({X: teX, Y: teY2,phase:False,g_filter:gaussian_kernel})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With one Layer\n",
    "We see that after epoch 0 the accuracy is high(skewed data set and the network might be predicting the same class), but then the accuracy decreases with epochs reaching a value around $\\textbf{88%}$. Moreover the cost function does not always decrease, this could mean that with the given parameters (learning rate, batch size etc), the optimization is $\\textbf{getting stuck}$ in a local minima. Thus the cost function is not steadily decreasing, leading to an acceptable performance. There is little difference between the training and testing accuracy, suggesting that the network is not overfitting, but has a high bias leading to an accuracy of $\\textbf{88%}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W1:0' shape=(3, 3, 1, 8) dtype=float32_ref>\n",
      "Cost after epoch 0: 0.162500 and training accuracy: 0.962127\n",
      "Cost after epoch 10: 0.121251 and training accuracy: 0.865745\n",
      "Cost after epoch 20: 0.160026 and training accuracy: 0.849600\n",
      "Cost after epoch 30: 0.189570 and training accuracy: 0.891000\n",
      "Cost after epoch 40: 0.210857 and training accuracy: 0.869891\n",
      "Cost after epoch 50: 0.226409 and training accuracy: 0.868600\n",
      "Cost after epoch 60: 0.239815 and training accuracy: 0.875400\n",
      "Cost after epoch 70: 0.245049 and training accuracy: 0.858818\n",
      "Cost after epoch 80: 0.245565 and training accuracy: 0.881145\n",
      "Cost after epoch 90: 0.242607 and training accuracy: 0.880982\n",
      "Train Accuracy: 0.87929094\n",
      "Test Accuracy: 0.8801\n"
     ]
    }
   ],
   "source": [
    "final_model(trX,trY,teX,teY,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Two Layers\n",
    "We see that after epoch 0 the accuracy is high(skewed data set and the network might be predicting the same class), but then the accuracy decreases with epochs reaching a value around $\\textbf{85%}$. Moreover the cost function does not always decrease, this could mean that with the given parameters (learning rate, batch size etc), the optimization is $\\textbf{getting stuck}$ in a local minima. Thus the cost function is not steadily decreasing, leading to poor performance. There is little difference between the training and testing accuracy, suggesting that the network is not overfitting, but has a high bias leading to an accuracy of $\\textbf{85%}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W1:0' shape=(3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'W2:0' shape=(3, 3, 8, 16) dtype=float32_ref>\n",
      "Cost after epoch 0: 0.098090 and training accuracy: 0.874218\n",
      "Cost after epoch 10: 0.215620 and training accuracy: 0.851527\n",
      "Cost after epoch 20: 0.619365 and training accuracy: 0.767764\n",
      "Cost after epoch 30: 0.734867 and training accuracy: 0.736564\n",
      "Cost after epoch 40: 0.739156 and training accuracy: 0.759127\n",
      "Cost after epoch 50: 0.709606 and training accuracy: 0.733673\n",
      "Cost after epoch 60: 0.686162 and training accuracy: 0.765818\n",
      "Cost after epoch 70: 0.652271 and training accuracy: 0.812873\n",
      "Cost after epoch 80: 0.614144 and training accuracy: 0.808764\n",
      "Cost after epoch 90: 0.574833 and training accuracy: 0.835691\n",
      "Train Accuracy: 0.84434545\n",
      "Test Accuracy: 0.8545\n"
     ]
    }
   ],
   "source": [
    "final_model(trX,trY,teX,teY,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Three Layers\n",
    "We see that after epoch 0 the accuracy is low, but then the accuracy increases with epochs reaching a value around $\\textbf{99%}$. Moreover the cost function not always decrease, this could mean that with the given parameters (learning rate, batch size etc), the optimization is $\\textbf{not getting stuck}$ in local minima. Thus the cost function is not steadily decreasing, leading to poor performance. There is little difference between the training and testing accuracy, suggesting that the network is not overfitting. This network is trained properly that gives the best result and leads to a high accuracy of $\\textbf{99%}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W1:0' shape=(3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'W2:0' shape=(3, 3, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'W3:0' shape=(3, 3, 16, 32) dtype=float32_ref>\n",
      "Cost after epoch 0: 0.146631 and training accuracy: 0.784164\n",
      "Cost after epoch 10: 0.010162 and training accuracy: 0.999709\n",
      "Cost after epoch 20: 0.008077 and training accuracy: 0.992055\n",
      "Cost after epoch 30: 0.122044 and training accuracy: 0.998600\n",
      "Cost after epoch 40: 0.097594 and training accuracy: 0.993000\n",
      "Cost after epoch 50: 0.078344 and training accuracy: 0.998255\n",
      "Cost after epoch 60: 0.063438 and training accuracy: 0.992964\n",
      "Cost after epoch 70: 0.054084 and training accuracy: 0.992400\n",
      "Cost after epoch 80: 0.046595 and training accuracy: 0.995345\n",
      "Cost after epoch 90: 0.040858 and training accuracy: 0.994945\n",
      "Train Accuracy: 0.9914182\n",
      "Test Accuracy: 0.9897\n"
     ]
    }
   ],
   "source": [
    "final_model(trX,trY,teX,teY,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best Result\n",
    "The best result is from Network 3 and the results are summarised below\n",
    "\n",
    "$$Train Accuracy =99.1\\%$$\n",
    "\n",
    "$$Test Accuracy = 98.9\\%$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
